# Qwen3

Qwen3æ¨¡å‹çš„éƒ¨ç½²ç¤ºä¾‹ï¼Œä»¥Qwen3-8Bä¸ºä¾‹ã€‚æš‚ä¸æ”¯æŒMoeæ¶æ„ï¼ˆQwen3-30B-A3Bã€Qwen3-235B-A22Bï¼‰ã€‚

## æ¼”ç¤º

<img src="gradio6.gif" alt="gradio6.gif">

## å¼€å‘ç¯å¢ƒ

è§[å¿«é€Ÿå¼€å§‹](../README.md#å¿«é€Ÿå¼€å§‹)çš„æ‹‰å–ä»£ç å’Œåˆ›å»ºå®¹å™¨éƒ¨åˆ†ã€‚å¦‚æœä¹‹å‰æ‹‰å–è¿‡é•œåƒï¼Œéœ€è¦é‡æ–°æ‹‰å–ä¸€ä¸‹ä½¿ç”¨æ–°çš„é•œåƒã€‚æ–°é•œåƒå¢åŠ äº†å¯¹Qwen3ForCausalLMçš„æ”¯æŒã€‚

```bash
# å®‰è£…è¾ƒæ–°ç‰ˆæœ¬transformersä¸modelopt
pip install git+https://github.com/huggingface/transformers@v4.51.0
pip install nvidia-modelopt==0.27.0
```

## æ„å»ºtrtllmå¼•æ“

```bash
# ä¸‹è½½Qwen3-8Bæ¨¡å‹
apt update && apt install git-lfs
git lfs install
git clone https://huggingface.co/Qwen/Qwen3-8B /tmp/Qwen3-8B

# è½¬æ¢ckpt
rm -rf /tmp/Qwen3-8B/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/qwen/convert_checkpoint.py --model_dir /tmp/Qwen3-8B \
--output_dir /tmp/Qwen3-8B/tllm_checkpoint/ --dtype bfloat16 --load_model_on_cpu

# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-8B/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-8B/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-8B/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256
# è¿è¡Œæµ‹è¯•
python3 third_party/TensorRT-LLM/examples/run.py --input_text "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ" --max_output_len=50 \
--tokenizer_dir /tmp/Qwen3-8B/ \
--engine_dir=/tmp/Qwen3-8B/trt_engines/
```

## é‡åŒ–

é€šè¿‡[NVIDIA Modelopt toolkit](https://github.com/NVIDIA/TensorRT-LLM/blob/v0.16.0/examples/quantization/README.md)
æ”¯æŒint8/int4 weight-onlyã€awqã€smooth quantã€fp8ç­‰é‡åŒ–ã€‚

### int8 weight-onlyé‡åŒ–

```bash
# è½¬æ¢ckpt
rm -rf /tmp/Qwen3-8B/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/quantization/quantize.py --model_dir /tmp/Qwen3-8B \
--dtype bfloat16 --qformat int8_wo \
--output_dir /tmp/Qwen3-8B/tllm_checkpoint/

# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-8B/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-8B/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-8B/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256
```

### int4-awqé‡åŒ–

```bash
# ä½¿ç”¨Modelopt è½¬æ¢ ckpt
rm -rf /tmp/Qwen3-8B/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/quantization/quantize.py --model_dir /tmp/Qwen3-8B \
--dtype bfloat16 --qformat int4_awq --awq_block_size 128 \
--output_dir /tmp/Qwen3-8B/tllm_checkpoint/
# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-8B/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-8B/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-8B/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256

# ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨huggingfaceçš„awqç‰ˆæœ¬ç›´æ¥è½¬æ¢ckptï¼Œä¾‹å¦‚Qwen3-14B-AWQ
git clone https://huggingface.co/Qwen/Qwen3-14B-AWQ /tmp/Qwen3-14B-AWQ
rm -rf /tmp/Qwen3-14B-AWQ/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/qwen/convert_checkpoint.py --model_dir /tmp/Qwen3-14B-AWQ \
--output_dir /tmp/Qwen3-14B-AWQ/tllm_checkpoint/ --dtype bfloat16 --load_model_on_cpu
# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-14B-AWQ/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-14B-AWQ/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-14B-AWQ/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256
```

### fp8é‡åŒ–

éœ€è¦ç¡¬ä»¶æ”¯æŒï¼Œå¦åˆ™ä¼šæœ‰å¦‚ä¸‹æŠ¥é”™ï¼š
```[TRT] [E] IBuilder::buildSerializedNetwork: Error Code 9: API Usage Error (Networks with FP8 Q/DQ layers require hardware with FP8 support.)```

```bash
# è½¬æ¢ckpt
rm -rf /tmp/Qwen3-8B/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/quantization/quantize.py --model_dir /tmp/Qwen3-8B \
--dtype bfloat16 --qformat fp8 --kv_cache_dtype fp8 --awq_block_size 128 \
--output_dir /tmp/Qwen3-8B/tllm_checkpoint/

# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-8B/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-8B/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-8B/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256
```

### smooth quanté‡åŒ–

```bash
# è½¬æ¢ckpt
rm -rf /tmp/Qwen3-8B/tllm_checkpoint/
python3 third_party/TensorRT-LLM/examples/quantization/quantize.py --model_dir /tmp/Qwen3-8B \
--dtype bfloat16 --qformat int8_sq --kv_cache_dtype int8 --awq_block_size 128 \
--output_dir /tmp/Qwen3-8B/tllm_checkpoint/

# æ„å»ºå¼•æ“
rm -rf /tmp/Qwen3-8B/trt_engines/
trtllm-build --checkpoint_dir /tmp/Qwen3-8B/tllm_checkpoint/ \
--output_dir /tmp/Qwen3-8B/trt_engines/ \
--gemm_plugin bfloat16 --max_batch_size 16 --paged_kv_cache enable --use_paged_context_fmha enable \
--max_input_len 32256 --max_seq_len 32768 --max_num_tokens 32256
```

## æ„å»ºä¸éƒ¨ç½²

### enable_thinking

Qwen3æ”¯æŒthinkingåŠŸèƒ½ï¼Œé»˜è®¤å¼€å¯ã€‚å¯ä»¥åœ¨[inference_qwen3.yml](../conf/inference_qwen3.yml)ä¸­ä¿®æ”¹`enable_thinking`
å‚æ•°ä¸ºfalseå…³é—­ï¼Œä¹Ÿå¯ä»¥åœ¨è¯·æ±‚ä¸­é€šè¿‡`enable_thinking`å‚æ•°å…³é—­ã€‚

```bash
# æ„å»º
grpst archive .

# éƒ¨ç½²ï¼Œ
# é€šè¿‡--inference_confå‚æ•°æŒ‡å®šæ¨¡å‹å¯¹åº”çš„inference.ymlé…ç½®æ–‡ä»¶å¯åŠ¨æœåŠ¡ã€‚
# å¦‚éœ€ä¿®æ”¹æœåŠ¡ç«¯å£ï¼Œå¹¶å‘é™åˆ¶ç­‰ï¼Œå¯ä»¥ä¿®æ”¹conf/server.ymlæ–‡ä»¶ï¼Œç„¶åå¯åŠ¨æ—¶æŒ‡å®š--server_confå‚æ•°æŒ‡å®šæ–°çš„server.ymlæ–‡ä»¶ã€‚
# æ³¨æ„å¦‚æœä½¿ç”¨å¤šå¡æ¨ç†ï¼Œéœ€è¦ä½¿ç”¨mpiæ–¹å¼å¯åŠ¨ï¼Œ--mpi_npå‚æ•°ä¸ºå¹¶è¡Œæ¨ç†çš„GPUæ•°é‡ã€‚
grpst start ./server.mar --inference_conf=conf/inference_qwen3.yml

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
grpst ps
# å¦‚ä¸‹è¾“å‡º
PORT(HTTP,RPC)      NAME                PID                 DEPLOY_PATH         
9997                my_grps             65322               /home/appops/.grps/my_grps
```

## æ¨¡æ‹Ÿè¯·æ±‚

```bash
# curlå‘½ä»¤éstreamè¯·æ±‚
curl --no-buffer http://127.0.0.1:9997/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
      }
    ]
  }'
# è¿”å›å¦‚ä¸‹ï¼š
: '
{
 "id": "chatcmpl-4",
 "object": "chat.completion",
 "created": 1746523087,
 "model": "qwen3",
 "system_fingerprint": "grps-trtllm-server",
 "choices": [
  {
   "index": 0,
   "message": {
    "role": "assistant",
    "content": "<think>\nå¥½çš„ï¼Œç”¨æˆ·é—®â€œä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿâ€ï¼Œæˆ‘éœ€è¦å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥ä»‹ç»è‡ªå·±çš„èº«ä»½ï¼Œè¯´æ˜æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œç”±é€šä¹‰å®éªŒå®¤ç ”å‘ã€‚ç„¶åï¼Œè¦æåˆ°æˆ‘çš„åŠŸèƒ½ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ç­‰ï¼Œè¿™æ ·ç”¨æˆ·èƒ½äº†è§£æˆ‘çš„èƒ½åŠ›èŒƒå›´ã€‚æ¥ç€ï¼Œå¯ä»¥è¯¢é—®ç”¨æˆ·æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„ï¼Œè¿™æ ·èƒ½å¼•å¯¼å¯¹è¯ç»§ç»­ä¸‹å»ã€‚åŒæ—¶ï¼Œä¿æŒè¯­æ°”å‹å¥½å’Œè‡ªç„¶ï¼Œé¿å…ä½¿ç”¨è¿‡äºæ­£å¼çš„è¯­è¨€ã€‚è¿˜è¦æ³¨æ„ä¸è¦åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œä¿æŒå›ç­”ç®€æ´ã€‚æœ€åï¼Œç¡®ä¿ç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾ï¼Œä¸ä½¿ç”¨Markdownæ ¼å¼ã€‚ç°åœ¨æŠŠè¿™äº›æ•´ç†æˆä¸€ä¸ªè‡ªç„¶æµç•…çš„å›ç­”ã€‚\n</think>\n\nä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ç­‰ï¼Œå¯ä»¥å¸®ä½ å®Œæˆå„ç§ä»»åŠ¡ã€‚æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼ŸğŸ˜Š"
   },
   "logprobs": null,
   "finish_reason": "stop"
  }
 ],
 "usage": {
  "prompt_tokens": 13,
  "completion_tokens": 178,
  "total_tokens": 191
 }
}
'

# curlå‘½ä»¤streamè¯·æ±‚
curl --no-buffer http://127.0.0.1:9997/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
      }
    ],
    "stream": true
  }'
# è¿”å›å¦‚ä¸‹ï¼š
: '
data: {"id":"chatcmpl-5","object":"chat.completion.chunk","created":1746523105,"model":"qwen3","system_fingerprint":"grps-trtllm-server","choices":[{"index":0,"delta":{"role":"assistant","content":"<think>"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chatcmpl-5","object":"chat.completion.chunk","created":1746523105,"model":"qwen3","system_fingerprint":"grps-trtllm-server","choices":[{"index":0,"delta":{"content":"\n"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chatcmpl-5","object":"chat.completion.chunk","created":1746523105,"model":"qwen3","system_fingerprint":"grps-trtllm-server","choices":[{"index":0,"delta":{"content":"å¥½çš„"},"logprobs":null,"finish_reason":null}]}
'

# æµ‹è¯•å…³é—­thinking
curl --no-buffer http://127.0.0.1:9997/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
      }
    ],
    "enable_thinking": false
  }'
# è¿”å›å¦‚ä¸‹ï¼š
: '
{
 "id": "chatcmpl-6",
 "object": "chat.completion",
 "created": 1746523170,
 "model": "qwen3",
 "system_fingerprint": "grps-trtllm-server",
 "choices": [
  {
   "index": 0,
   "message": {
    "role": "assistant",
    "content": "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é€šä¹‰å®éªŒå®¤å¼€å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€å¤šè¯­è¨€ç†è§£ã€ä»£ç ç¼–å†™ç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼"
   },
   "logprobs": null,
   "finish_reason": "stop"
  }
 ],
 "usage": {
  "prompt_tokens": 17,
  "completion_tokens": 62,
  "total_tokens": 79
 }
}
'

# openai_cli.py éstreamè¯·æ±‚
python3 client/openai_cli.py 127.0.0.1:9997 "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ" false
# è¿”å›å¦‚ä¸‹ï¼š
: '
ChatCompletion(id='chatcmpl-7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\nå¥½çš„ï¼Œç”¨æˆ·é—®â€œä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿâ€ï¼Œæˆ‘éœ€è¦å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥ä»‹ç»è‡ªå·±çš„èº«ä»½ï¼Œè¯´æ˜æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œç”±é€šä¹‰å®éªŒå®¤ç ”å‘ã€‚ç„¶åï¼Œè¦æåˆ°æˆ‘çš„åŠŸèƒ½ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ç­‰ï¼Œè¿™æ ·ç”¨æˆ·èƒ½äº†è§£æˆ‘çš„èƒ½åŠ›èŒƒå›´ã€‚æ¥ç€ï¼Œå¯ä»¥è¯¢é—®ç”¨æˆ·æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„ï¼Œè¿™æ ·èƒ½å¼•å¯¼å¯¹è¯ç»§ç»­ä¸‹å»ã€‚åŒæ—¶ï¼Œä¿æŒè¯­æ°”å‹å¥½å’Œè‡ªç„¶ï¼Œé¿å…ä½¿ç”¨è¿‡äºæ­£å¼çš„è¯­è¨€ã€‚è¿˜è¦æ³¨æ„ä¸è¦åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œä¿æŒå›ç­”ç®€æ´ã€‚æœ€åï¼Œç¡®ä¿ç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾ï¼Œä¸ä½¿ç”¨Markdownæ ¼å¼ã€‚ç°åœ¨æŠŠè¿™äº›æ•´ç†æˆä¸€ä¸ªè‡ªç„¶æµç•…çš„å›ç­”ã€‚\n</think>\n\nä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ç­‰ï¼Œå¯ä»¥å¸®ä½ å®Œæˆå„ç§ä»»åŠ¡ã€‚æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼ŸğŸ˜Š', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1746523198, model='', object='chat.completion', service_tier=None, system_fingerprint='grps-trtllm-server', usage=CompletionUsage(completion_tokens=178, prompt_tokens=13, total_tokens=191, completion_tokens_details=None, prompt_tokens_details=None))
'

# openai_cli.py streamè¯·æ±‚
python3 client/openai_cli.py 127.0.0.1:9997 "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ" true
# è¿”å›å¦‚ä¸‹ï¼š
: '
ChatCompletionChunk(id='chatcmpl-8', choices=[Choice(delta=ChoiceDelta(content='<think>', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1746523220, model='', object='chat.completion.chunk', service_tier=None, system_fingerprint='grps-trtllm-server', usage=None)
ChatCompletionChunk(id='chatcmpl-8', choices=[Choice(delta=ChoiceDelta(content='\n', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1746523220, model='', object='chat.completion.chunk', service_tier=None, system_fingerprint='grps-trtllm-server', usage=None)
ChatCompletionChunk(id='chatcmpl-8', choices=[Choice(delta=ChoiceDelta(content='å¥½çš„', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1746523220, model='', object='chat.completion.chunk', service_tier=None, system_fingerprint='grps-trtllm-server', usage=None)
'

# è¾“å…¥32ké•¿æ–‡æœ¬å°è¯´éªŒè¯é•¿æ–‡æœ¬çš„æ”¯æŒ
python3 client/openai_txt_cli.py 127.0.0.1:9997 ./data/32k_novel.txt "ä¸Šé¢è¿™ç¯‡å°è¯´ä½œè€…æ˜¯è°ï¼Ÿ" false
# è¿”å›å¦‚ä¸‹ï¼š
: '
ChatCompletion(id='chatcmpl-9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\nå¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦åˆ†æç”¨æˆ·æä¾›çš„å°è¯´å†…å®¹ï¼Œå¹¶å›ç­”å…³äºä½œè€…çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™å‡ºçš„å°è¯´æ ‡é¢˜æ˜¯ã€Šæ‹œæ‰˜ï¼Œåªæƒ³å¹²é¥­çš„åŒ—æç†Šè¶…é…·çš„ï¼ã€‹ï¼Œä½œè€…ç½²åæ˜¯å¼¦ä¸‰åƒã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯è¯¢é—®è¿™éƒ¨å°è¯´çš„ä½œè€…æ˜¯è°ï¼Œä½†å¯èƒ½ç”¨æˆ·è¯¯ä»¥ä¸ºéœ€è¦æ›´æ·±å…¥çš„åˆ†æï¼Œæ¯”å¦‚ä½œè€…çš„å†™ä½œé£æ ¼ã€ä½œå“ç‰¹ç‚¹ç­‰ã€‚\n\né¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ç”¨æˆ·çš„é—®é¢˜æ˜¯å¦ç›´æ¥ã€‚ç”¨æˆ·å¯èƒ½åªæ˜¯æƒ³ç¡®è®¤ä½œè€…åå­—ï¼Œä½†æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°æ›´è¯¦ç»†çš„åˆ†æã€‚ä¸è¿‡æ ¹æ®é—®é¢˜æè¿°ï¼Œç”¨æˆ·æœ€åæåˆ°â€œä½œè€…æ˜¯è°ï¼Ÿâ€ï¼Œæ‰€ä»¥å¯èƒ½åªæ˜¯éœ€è¦ç¡®è®¤ä½œè€…åå­—ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦æ£€æŸ¥æä¾›çš„æ–‡æœ¬ä¸­æ˜¯å¦æœ‰å…¶ä»–çº¿ç´¢ã€‚ä¾‹å¦‚ï¼Œå°è¯´çš„å¼€å¤´éƒ¨åˆ†æåˆ°â€œä½œè€…ï¼šå¼¦ä¸‰åƒâ€ï¼Œè¿™å¯èƒ½æ„å‘³ç€ä½œè€…çš„åå­—æ˜¯å¼¦ä¸‰åƒã€‚ä½†éœ€è¦ç¡®è®¤æ˜¯å¦æœ‰å…¶ä»–ä¿¡æ¯å¯èƒ½å½±å“åˆ¤æ–­ã€‚\n\næ­¤å¤–ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›äº†è§£å¼¦ä¸‰åƒçš„å…¶ä»–ä½œå“æˆ–å†™ä½œé£æ ¼ï¼Œä½†æ ¹æ®å½“å‰ä¿¡æ¯ï¼Œåªèƒ½ç¡®è®¤ä½œè€…åå­—ã€‚å› æ­¤ï¼Œå›ç­”åº”æ˜ç¡®æŒ‡å‡ºä½œè€…æ˜¯å¼¦ä¸‰åƒï¼Œå¹¶å¯èƒ½ç®€è¦è¯´æ˜å…¶ä½œå“ç‰¹ç‚¹ï¼Œå¦‚è½»æ¾å¹½é»˜ã€èŒå® å…ƒç´ ç­‰ï¼Œä»¥ç¬¦åˆç”¨æˆ·æä¾›çš„æ ‡ç­¾å’Œå†…å®¹ã€‚\n\néœ€è¦ç¡®ä¿å›ç­”å‡†ç¡®ï¼Œä¸æ·»åŠ æœªè¯å®çš„ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä¿æŒå›ç­”ç®€æ´ï¼Œç¬¦åˆç”¨æˆ·å¯èƒ½çš„éœ€æ±‚ã€‚\n</think>\n\nã€Šæ‹œæ‰˜ï¼Œåªæƒ³å¹²é¥­çš„åŒ—æç†Šè¶…é…·çš„ï¼ã€‹çš„ä½œè€…æ˜¯**å¼¦ä¸‰åƒ**ã€‚  \nè¯¥ä½œå“å±äºè½»æ¾ç”œå® é£æ ¼ï¼Œèåˆäº†ç©¿ä¹¦ã€èŒå® ã€æ²»æ„ˆç­‰å…ƒç´ ï¼Œä»¥åŒ—æç†Šæ¥šäº‘éœçš„è§†è§’å±•å¼€å¥‡å¹»å†’é™©ï¼Œå‰§æƒ…æ¸©é¦¨æœ‰è¶£ï¼Œå……æ»¡ç”Ÿæ´»åŒ–çš„å°ç»†èŠ‚å’ŒåŠ¨ç‰©äº’åŠ¨ï¼Œå±•ç°äº†ä¸»è§’åœ¨å¼‚ä¸–ç•Œç”Ÿå­˜çš„æ—¥å¸¸ä¸æˆé•¿ã€‚å¼¦ä¸‰åƒçš„æ–‡é£é€šå¸¸ä»¥ç»†è…»çš„æå†™å’Œå¹½é»˜çš„å™äº‹è§é•¿ï¼Œé€‚åˆå–œæ¬¢æ²»æ„ˆç³»ã€èŒå® é¢˜æçš„è¯»è€…ã€‚', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1746523240, model='', object='chat.completion', service_tier=None, system_fingerprint='grps-trtllm-server', usage=CompletionUsage(completion_tokens=370, prompt_tokens=31594, total_tokens=31964, completion_tokens_details=None, prompt_tokens_details=None))
'

# openai_func_call.pyè¿›è¡Œfunction callæ¨¡æ‹Ÿ
python3 client/openai_func_call.py 127.0.0.1:9997
# è¿”å›å¦‚ä¸‹ï¼š
: '
Query server with question: What's the weather like in Boston today? ...
Server response: thought: None, call local function(get_current_weather) with arguments: location=Boston, MA, unit=fahrenheit
Send the result back to the server with function result(59.0) ...
Final server response: <think>
Okay, the user asked about the weather in Boston today. I called the get_current_weather function with Boston, MA and Fahrenheit as the unit. The response came back as 59.0. Now I need to present this information clearly.

First, I should state the temperature in a friendly manner. Since it's 59Â°F, that's a mild temperature, maybe mention it's comfortable. Also, include the unit to avoid confusion. Keep it simple and straightforward. Let me check if there's any additional info needed, but the function only provided the temperature. So just report that and maybe add a note about it being a pleasant day. Alright, that should cover it.
</think>

The current temperature in Boston, MA is 59.0Â°F. That's a comfortable, mild day!
'

# openai_func_call2.pyè¿›è¡Œä¸€æ¬¡ä¸¤ä¸ªå‡½æ•°çš„function callæ¨¡æ‹Ÿ
python3 client/openai_func_call2.py 127.0.0.1:9997
# è¿”å›å¦‚ä¸‹ï¼š
: '
Query server with question: What's the postcode of Boston and what's the weather like in Boston today? ...
Server response: thought: None, call local function(get_postcode) with arguments: location=Boston
Server response: thought: None, call local function(get_current_weather) with arguments: location=Boston, unit=fahrenheit
Send the result back to the server with function result ...
Final server response: <think>
Okay, let's see. The user asked for the postcode of Boston and the weather there. First, I called get_postcode with Boston as the location. The response was 02138. Then I called get_current_weather with Boston and Fahrenheit. The response was 59.0. Wait, the weather response is just a temperature? The user probably expects more details like conditions, humidity, etc. But the tool's response only gave the temperature. Maybe the tool's description is incomplete. Anyway, I need to present the info I have. So, the postcode is 02138, and the temperature is 59Â°F. I should mention that the weather data is limited to temperature. Let me check if I need to ask for more details, but since the tools are fixed, I can't. So, I'll format the answer with the postcode and the temperature, noting the limitation.
</think>

The postcode for Boston is **02138**.

The current temperature in Boston is **59.0Â°F**. Note that the weather tool provided only the temperature, and additional details like conditions or humidity were not included. Let me know if you'd like further assistance!
'

# llama-index ai agentæ¨¡æ‹Ÿ
pip install llama_index llama_index.llms.openai_like
python3 client/llamaindex_ai_agent.py 127.0.0.1:9997
# è¿”å›å¦‚ä¸‹ï¼š
: '
Query: What is the weather in Boston today?
Added user message to memory: What is the weather in Boston today?
=== Calling Function ===
Calling function: get_weather with args: {"location":"Boston, MA","unit":"fahrenheit"}
Got output: 59.0
========================

Response: <think>
Okay, the user asked for the weather in Boston today. I called the get_weather function with location set to "Boston, MA" and unit as "fahrenheit". The response came back as 59.0. Now I need to present this information in a clear and friendly way.

First, I should mention the temperature, making sure to include the unit which is Fahrenheit. Since 59Â°F is a bit cool, maybe add a note about it being a mild day. Also, the user might appreciate a suggestion, like recommending a light jacket. Keep the response concise but helpful. Let me check if there's any other info needed, but since the function only provided the temperature, stick to that. Make sure the answer is natural and not too robotic.
</think>

The current temperature in Boston, MA is 59.0Â°F. That's a mild day, perfect for a light jacket! ğŸŒ¤ï¸
'
```

## å¼€å¯gradioæœåŠ¡

```bash
# å®‰è£…gradio
pip install -r tools/gradio/requirements.txt

# å¯åŠ¨çº¯æ–‡æœ¬èŠå¤©ç•Œé¢ï¼Œqwen3æ”¯æŒCoTè¿‡ç¨‹çš„çº¯æ–‡æœ¬èŠå¤©ï¼Œ0.0.0.0:9997è¡¨ç¤ºllmåç«¯æœåŠ¡åœ°å€
python3 tools/gradio/llm_app.py qwen3 0.0.0.0:9997
```

## å…³é—­æœåŠ¡

```bash
# å…³é—­æœåŠ¡
grpst stop my_grps
```