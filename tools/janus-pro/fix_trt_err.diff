diff --git a/janus/models/siglip_vit.py b/janus/models/siglip_vit.py
index ba426d6..bd2377c 100644
--- a/janus/models/siglip_vit.py
+++ b/janus/models/siglip_vit.py
@@ -152,7 +152,7 @@ class Attention(nn.Module):
         self.head_dim = dim // num_heads
         self.scale = self.head_dim**-0.5
         # self.fused_attn = use_fused_attn()
-        self.fused_attn = True
+        self.fused_attn = False
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
         self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
